---
title: Class Project - High Culture and Popular Culture in Book Reviews
date: '2020-03-27'
permalink: /posts/2020/03/PSYC40460/
tags:
  - Class Project
---

Project for PSYC40460 (Computation and the Identification of Cultural Patterns). Analyzing online book reviews in the framework of High/Popular Culture.

# Disclaimer

The book reviews were scraped from [goodreads.com](https://www.goodreads.com/). Although we are not uploading any scraped data, I will gladly remove this material from my website if there is any problem in regards to the copyright, etc.

# Introduction

Upon walking into a bookstore and roaming around the fiction section, one can usually notice an interesting phenomenon. Within the section for "fictions," there are usually shelves specifically reserved for "classics" or "literature." 

What distinguishes fiction labeled this way from other unlabeled fiction? An important theory that gives insight into this question is Bourdieu’s theory of taste. In his renowned book _Distinction: A Social Critique of the Judgement of Taste,_ Bourdieu (1979/1984) claimed that there are socially recognized hierarchies among arts, and this hierarchy is correlated with the hierarchy of social classes. While members of the dominant class gain knowledge to understand the sophisticated and complex codes of arts positioned high in the hierarchy through education, the members of the subordinate class are denied of this knowledge (Blewitt, 1993). This leads to the distinction between the “high culture” which is esoteric and circulated within the higher classes, and the “popular culture”, defined here as the residual of the high culture or culture that does not meet the required criterion to be considered as high culture (Storey, 2009), which is simple, barbarous and is enjoyed by the mass lower class. 

Indeed, fiction has always been one of the core mediums of this distinction between high culture and popular culture. Reading important literature works had often been a core curriculum of prestigious schools. For example, numerous universities operated the “great-books program” focusing on reading, discussing, and writing about the “great-books” during the 20th century, and many universities are still operating a similar program (Casement, 2002). In addition, there were frequent attempts to compile a list of so-called “cannons” of fiction to clarify what fictions most definitely belong to the high culture. With Bloom (1994)’s famous _The Western Canon: The Books and School of the Ages_ leading the charge, many scholars and companies (such as the [TIME magazine](https://entertainment.time.com/2005/10/16/all-time-100-novels/) or [Modern Library](http://www.modernlibrary.com/top-100/100-best-novels/)) created a list of “classics” that are important in the high culture.

Based on this background, the current project tries to see if we can empirically see this distinction in people's cultural patterns toward books of high culture and books of popular culture through the scope of book reviews. My research question is “Do people express different cultural patterns when reviewing books in high culture and popular culture?” More specifically, I want to find out if we can answer these questions: 1) Do people express more signs of high classes when reviewing books of high culture? (e.g., using more difficult words, referring more to the authors) 2) Is there a topic (or topics) that are only selectively seen in one category of books?

Additionally, I want to apply some black-box methodologies to see if we can 1) build a model that can classify reviews about high culture books and 2) build a model that can generate texts that resemble the reviews. It will be unclear what the results mean since these methods are not easily interpretable even to the people who made the methods (Rahwan et al., 2019). However, it will serve as an indication that there is 1) indeed a distinction between the two categories and 2) the patterns are at least partially learnable by algorithms.

# Data

## Data Collection

To answer my research question, I will need three sources of data. One data is a list of fictions that belong to the high culture. A natural source of this kind of list will be the lists compiled by scholars and publishing companies that were mentioned in the introduction. I had gone through several lists of such (with the help of a list provided by a private website called [the Greatest Books](https://thegreatestbooks.org/lists/details)) and decided to use the [Modern Library's list of best 100 novels of the century](http://www.modernlibrary.com/top-100/100-best-novels/) for multiple reasons. The most important reason to choose this list was that the Modern Library was a company that was specifically created to publish "World’s Best Books" (About Modern Library, n.d.) and had been selecting and publishing "Best Books" since the 1920s. The long history of compiling such lists and the relatively large number of board members in making the list (five members) ensured me that this list is reliable. Also, this list is quite famous and even has its own [Wikipedia article](https://en.wikipedia.org/wiki/Modern_Library_100_Best_Novels), suggesting that there will be many people trying out the books on the list and writing reviews about it. Finally, this list only included books that were written in a specific language (English) and a specific time (20th century), which will be helpful to reduce the variance of the data. Out of the 100 books on the list, reviews for 9 books were not collected since they also appeared in the Publisher Weekly Bestseller list (see the section "Crosschecking and Limitation of the Data" for further explanation).

The other source of data is a list of fictions that belong to the popular culture. By definition, this list has to be a list of books that were widely read and enjoyed by the mass public. Although being enjoyed by the public does not necessarily mean that that book is sold a lot, I think being sold a lot could be a strong indication that a book was popular. Therefore, I decided to generate a list from the New York Times Bestseller List, which publishes a list of books most sold in America weekly. I made around 500 calls to the [The New York Times books api](https://developer.nytimes.com/docs/books-product/1/overview) and compiled a list of best sellers from 2011 to the current year (Although the Best Seller List had been published since the 1930s, the API only gave me the information from 2011). Then, the list was sorted by rating calculated as $$\Sigma_{i=1}^{k}(16 - rank_i)$$ where $k$ is the number of calls to API and $rank_i$ is the rank the book had in a call $i$ ($rank_i$ = 0 when the book did not appear in call $i$). For example, if a novel appeared only once on the Best Sellers List and had was ranked 10th that week, it will have a score of 6 (16 - 10). After sorting the list, I selected the first 102 books in the list excluding 2 books written by Margaret Atwood (see the section "Crosschecking and Limitation of the Data" for further explanation) resulting in a list of 100 books.

Finally, I will need a data source where I can get people's reviews of these books. The data source I am going to use for this purpose is the website [Goodreads](https://www.goodreads.com/). The platform has over 90 million users and over 90 million reviews on 2.6 billion books (About Goodreads, n.d.). Individual books have their own pages on this website, and people can post their reviews on that book or see other people's reviews about that book on that page. Goodreads has its own API to provide data to the developers, but it only provided the first 300 characters of the top 100 reviews of each book. Therefore, I decided that I would build a scraper (based on python and selenium) to get the reviews. Even with the scraper, I was not able to get all the reviews of the books, since Goodreads only make the top 300 reviews public for each language. I had finished scraping English reviews for all 191 books in the two lists mentioned above.

## Data Cleaning

After getting the data, I cleaned the data in a way that all characters are converted to lower cases and everything that was not characters, digits, or certain punctuations (.,!?'-) was removed. Also, upon inspecting the data, I found that many reviews were written without reading the books or was more of a short comment than a proper book review. Therefore, I decided to analyze reviews that were longer than 250 characters after cleaning. I do not think this is a harsh criterion, especially considering that you can write 280 characters in a tweet. This process dropped around 20% of the data, resulting in final data size of 44,927 reviews. In addition, I lemmatized and tokenized the reviews (excluding stop words and digits).

## Limitations of the Data and Crosschecking

One important limitation of the data comes from the fact that the distinction between high culture and popular culture is not as clear as we want it to be. Bennett (1980) even said that "the concept of popular culture is virtually useless, a melting pot of confused and contradictory meanings" (p.18). Although we choose the narrow definition of popular culture as the residual of high culture, there are always cases in which a piece of art could be seen as a part of high culture and as a part of popular culture simultaneously. For example, Harper Lee's _To Kill a Mockingbird_ is often considered to be one of the classics of 20th century (it is included in the list TIME Magazine created as the 100 best novels, which is another popular list of literary classics) but it stayed in the New York Times Best Seller List for 98 weeks (Murphy, 2015, July 16). Also, whether a piece of art is seen as in high culture or popular culture can change as time passes. For example, although Shakespeare's play is seen as almost the paragon of high culture in the modern era, Shakespeare's play received mass popularity in the 19th century (Storey, 2009; Levine 1990).

In order to take account of this fuzziness and overlap between high culture and popular culture, I first crosschecked the list of books in the Modern Library list with the [Publishers Weekly list of bestselling novels in the United States uploaded in Wikipedia](https://en.wikipedia.org/wiki/Publishers_Weekly_list_of_bestselling_novels_in_the_United_States_in_the_1900s) from 1900 to 1999. The reason for using the Publishers Weekly list instead of the New York Times list was because the New York Times website only showed the bestseller list from 2008, while the Publishers Weekly list was well documented in Wikipedia. Out of about 10,000 bestselling novels in Publishers Weekly, 9 books were also included in the Modern Library list (Lolita, Tropic of Cancer, The Naked and the Dead, Portnoy's Complaint, The Age of Innocence, Main Street, The House of Mirth, Ragtime, Sophie's Choice). These 9 books were excluded from analysis, since appearing in both list implied that these books potentially could be included in both cultures.

In addition, I crosschecked the authors of books from the New York Times list with winners of [Nobel Prize in Literature (1901-2019)](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Literature), [the Booker Prize (1969-2019; also the shortlisted authors were checked in this case)](https://en.wikipedia.org/wiki/List_of_winners_and_shortlisted_authors_of_the_Booker_Prize), [America Reward in Literature (1994-2017)](https://en.wikipedia.org/wiki/America_Award_in_Literature), [Neustadt International Prize for Literature (1970-2020)](https://en.wikipedia.org/wiki/Neustadt_International_Prize_for_Literature), [Franz Kafka Prize (2001-2019)](https://en.wikipedia.org/wiki/Franz_Kafka_Prize), and [International Booker Prize (2005-2019)](https://en.wikipedia.org/wiki/International_Booker_Prize) in Wikipedia to see if there is any overlap. The logic behind comparing the list with literature awards instead of another kind of classics list was that it usually takes a very long time for the scholars to reach a consensus of what author or book should be seen as a classic of an era. Since all the books on the popular culture list were published after 2011, it was highly unlikely that any of these books appeared in a list of such. Literary awards were thought of as an alternative to the classics list because the most prestigious awards in literature are targeted toward the novels in the higher hierarchy. The reason for using authors, not novels themselves, was because most literary awards were given to an author, not specific books. The only author that appeared both in the bestseller list and any of the awards mentioned above was Margaret Atwood, who won the Booker Prize. There were two books written by her in the top 100, so the reviews for those two reviews were not analyzed.

Although I crosschecked the lists and removed books that might be belonging both to the two cultures, it will be impossible to say that these steps had controlled for the overlap between high culture and popular culture. Even in the theoretical background, there is an inherent blur in the distinction between the two. It will be beyond the scope of this project to perfectly distinguish the two. Therefore, the results from this project should be read with caution that the distinction between high culture and popular culture is as clear as we want it to be.

Another potential limitation to the data could be that there is a glaring time discrepancy between my list of books belonging to high culture (20th century) and my list of books belonging to popular culture (2011-2020). I do not think that this is a big problem since Goodreads started operating in 2007, so all the reviews for the high culture books would have been written in time relatively close to the reviews for the popular culture books were written. However, it is worth noting that this time discrepancy might be affecting the analysis in some way.

Finally, it is important to note that this is an analysis targeted at the reviews of the novels, not the text itself. This analysis is about how the people perceive and review the books in both cultures, not the characteristics of the books belonging to each culture itself. For example, saying that the reviews in books belonging to high culture have certain characteristics does not mean that books belonging to high culture have that characteristic. Also, based on the fact that this website is open to everyone without strict restriction and that the number of users on the website is great (90 million), it is highly likely that the population in this group is more representative of the lower hierarchy in Bourdieu's view. Therefore, this analysis is largely on how the general public (in the lower hierarchy) views the novels in each category, not the quality or characteristics of the books themselves.

# Are people actually writing book reviews?

We have already shown that there are indeed a lot of reviews on this specific platform that follows the platform’s rules of posting reviews. There were a total of 3,070,623 reviews attached to 191 books we collected review for (although we are only going to analyze 44,927 reviews, as discussed in the previous sections), and this at least corroborates that there is a cultural phenomenon that makes people generate some texts adhering to rule of a website.

However, this number itself does not tell us that these reviews were talking about books. For all we know, people might be using this website as some sort of trolling page where people post all sorts of weird things. One way to find out if these reviews are actually about books is to see if the comments include words that are related to books and reading. Based on this thought, I first checked whether each review contains some words closely related to books and reading: “book”, “read”, “novel”, “story”.

I found that about 96.7% of the reviews included at least one word that is closely related to books and reading. I think this data is sufficient to show that the massive activity on the website is indeed related to books and reading. However, we are not sure whether people are expressing feelings and opinions about books and reading yet. Another way to see what kind of things people are talking about will be to see what words frequently appear in the reviews. Following plots show 20 lemmas most frequently occur in the reviews from the classics and the reviews from the bestsellers:

![figure1]()
![figure2]()

The above plots show that the most common words that people used include words such as “like”, “love”, “think” alongside the words we had discussed above ("book", "read", "novel", "story") in both datasets. Although the relationship is relatively weak, these words indicate that people are expressing some sort of feelings and opinions. Combined with the result discussed above and the fact that words closely related to novels like “story” and “character” could be found in both plots, I think this shows that people are expressing feelings and thoughts about books over and over. This could be seen as empirical evidence that each review is indeed a replica of the legisign with the same interpretants and sign relations.

Finally, another interesting thing to see is whether people directly index the book they are talking about. As previously mentioned, Goodreads have a separate page for each book. Therefore, when commenting on a page about a specific book, people could automatically index the book they are expressing feelings and opinions about without explicitly mentioning them. For example, when talking about _the Sound and the Fury,_ people could just say “this book” instead of spelling out the entire book name to index the book. Therefore, it is unlikely that people will mention the title of the book directly. However, if a lot of people actually do mention the specific book that they are talking about, it could corroborate that the replica of legisigns we are talking about is indeed specific to each book. I tested this possibility by calculating the percentage of reviews that explicitly mentioned the title of the books. About 34.6% of the reviews mentioned the title of the book they are reviewing explicitly in their review. This is not a high percentage, but I think this could be some supplementary information.

# What Are The Difference Between The Two Categories?

## Goodreads Stats

Since it had been clear that people are writing book reviews in Goodreads, let's jump into the main research question - can we find different cultural patterns depending on the category of the books people are reviewing? The main focus of this analysis will be on the text analysis of book reviews, but there are also some numbers that Goodreads itself gives. For each book, Goodreads gives the average rating users gave to the book, the number of ratings that users gave to the book and the number of reviews that users wrote about the book. If we assume that the group of users who rates and write reviews about books in Goodreads are from the lower class, Bourdieu's theory predicts that books in the popular culture will get higher ratings than the books in the high culture. The lower class does not have the knowledge and education to grasp the value of books in high culture fully, while they will be able to capture the quality of books in popular culture better. Therefore, we could expect that books in high culture will have lower ratings than books in popular culture. Also, since it is more accessible, we could expect that books in popular culture will have a higher number of ratings and reviews.

Following boxplots show the three values for the books in each category:
![Figure3]()

t-tests showed that there were significant differences between the two categories in average rating (t=6.81, p<.001), number of rating (t=1.73, p<.001), and number of reviews (t=6.39, 1.27). However, do note that the enormous number of data points we have will make anything significant in the Null-Hypothesis testing framework. 

We can see that the results are in line with the expectations. The ratings for the books in high culture were significantly lower than the ratings for the books in popular culture, and there were significantly more reviews for the books in popular culture than reviews for the high culture. Also, although the result is only marginally significant, the number of ratings for the books in popular culture was higher than the number of ratings for the books in high culture.

These results are not the core analysis of this project, but they serve as a sanity check for the data. From the significant results, we can confirm that there are some differences between the two categories even at a very simple level. Also, since the results matched the expectation, it indirectly indicates that the distinction between the two categories is valid.

## Readability Score: Are People Writing More Difficult Reviews For High Culture Books?

We will now start to see if there is a difference between the categories at the review level. The first specific research question I introduced in the introduction was "Do people express more signs of high classes when reviewing books of high culture?" One sign of high class in verbal communication could be that people in higher classes use more complex words and sentence structures. For example, many developmental psychology studies suggest that children who grew up in lower socioeconomic status tend to be exposed to fewer vocabularies (e.g. Hart & Risley, 1995; Hoff, 2003; Rowe, 2008; Gilkerson et al, 2017). Also, studies show that vocabulary size increases as students go through education, even for university students (Milton & Treffers-Daller, 2013). Therefore, if the people reviewing the high culture books are from higher classes that gained relatively more cultural capital through educations, the language they use will be more complicated than the reviews for the popular culture books. 

One way to test such a relationship will be to use readability metrics. Readability metrics give quantitative estimates of how difficult it is to read texts (Friedman & Hoffman-Goetz, 2006). For example, the Coleman-Liau index (Coleman& Liau, 1975) calculates the readability of a text by calculating the average number of sentences and the average number of letters per 100 words, and estimate the level of education (in U.S. Grade level) needed to comprehend it. If we apply readability metrics to the reviews and find that reviews for the high culture books are harder to read than reviews for popular culture books, then we can argue that the relationship discussed above holds.

I have applied 7 conventional readability index (The Flesch Reading Ease formula, the Flesch-Kincaid Grade Level, the Gunning FOG Formula, the Automated Readability Index, the Coleman-Liau Index, the Linsear Write Formula, and the Dale-Chall Readability Score) using the `textstat` package. In addition, I used the `textstat` packages `text_standard` measure which averages all metrics' results and gives you an estimate of the school grade level required to comprehend the text.

The followings are the boxplots showing the results of the metrics. Note that I discarded reviews that show a score that is ± 2 Standard Deviation from the mean for each metric. I also plotted the number of lexicons of review since I had one slot left, but this metric is not very useful since it does not normalize the length of the sentences or words. 
![Figure4]()

t-tests showed that there were significant differences between the two categories in all readability metrics (the Flesch Reading Ease formula: t=-53.49, p<.001; the Flesch-Kincaid Grade Level: t=45.66, p<.001; the Gunning FOG Formula: t=47.54, p<.001; the Automated Readability Index: t=29.78, p<.001; the Coleman-Liau Index: t=51.76, p<.001; the Linsear Write Formula: t=35.64, p<.001, the Dale-Chall Readability Score: t=54.49, p<.001). Again, do note that the enormous number of data points we have will make anything significant in the Null-Hypothesis testing framework. 

This suggests that the semiotic relationship discussed above holds. To get a brief understanding of how large the difference is, let's use the `text_standard` value to find the average grade level required to comprehend reviews for each category. The average grade level required to comprehend reviews on higher culture books is about 8.6 grades and the average grade level required to comprehend reviews on popular culture books is about 7.4 grades. In other words, there is about 1.2 grade level difference between the two.

## Alternative Explanation for Readability Metrics Results

I concluded above that readability metrics indicating reviews for high culture books are more difficult to read is a result that supports the semiotic relationship of reviewers indexing that they are educated. However, it will be easy to claim that the exact opposite thing is happening. Being difficult to read could mean that people are using complicated words and eloquent sentence structure, but it could also mean that they are just writing terribly. In many cases, writing succinctly is recommended in both academic and real-life settings. If the reviewers are writing lengthy sentences that are hard to understand (remember that readability metrics do not account for grammar), then the semiotic relationship will be actually the opposite. The hard-to-read sentences (Sign-Vehicle) will index having less writing skills and fewer educations, which will form the Interpretant that could be expressed in the proposition "The writer of this review was not educated in writing much".

However, this explanation will not hold if we find that the reviews for high culture books have more sophisticated topics and contents. If we can identify some patterns that the reviews for high cultures are using more specific language when talking about books and talks more about literary elements of the books, we can see that they indeed are more educated or experience in discussing novels. This will show that the increase in difficulty in reading is a byproduct of more educated topics and languages, not results of bad writings. The following sections will show that this is indeed what is happening, ruling out this alternate explanation.

## Novels versus Books: Are people using more specific terms when reviewing high culture books?

To demonstrate that reviews for high culture books are indeed speaking a more educated language, I will start with a very subtle distinction between "book" and "novel". Although these two terms essentially index the same thing in the context of a review, there is a minor but important difference between the two. “Book” is a more general term including a variety of texts from picture books to philosophy texts. “Novel” is a term that specifically points to a certain genre of books. Therefore, I think pointing the book that is being reviewed as “novel” instead of "book" can softly indicate that this person has some background in this specific genre, and is discussing the book on a specific ground. To see if there is a significant difference, I calculated the ratio of reviews that contained the word “book” and “novel” separately for all 191 novels. Following boxplot shows the result:

![Figure5]()

The above plot and t-tests show that the ratio of reviews mentioning “novel” was significantly higher in high culture books than in popular culture books (t=-8.47, p<.001), and the reverse were true for the ratio of reviews mentioning “book” (t=9.95, p<.001). This shows that even the subtle difference between the two words could be shown as empirically shown in this dataset.

## Authors: Are people talking more about the transcendental figure behind the texts when reviewing high culture books?
As a transitional bridge between the word-level difference ("book" vs "novel") and clusters of words as topics (discussed in the next section), let's see if we can find a word-level difference that can signify that a construct is more widely accessed in the reviews for high culture books - the construct of author/auther. Michel Foucault claims that 'author' is not the mere writer of the text nor a human being behind the pen - rather, it is a transcendental figure or a name behind the texts (Foucault, 1979; Nehamas, 1986). Roughly put, in traditional literary criticism, the author is considered as an entity that knows the meaning of the text, and the criticism revolving around the author to interpret what the author meant, treating the text as questions with answers. Although Foucault concludes that this figure can be and should be decomposed, he does points out that "'literary' discourse was acceptable only if it carried an author's name" (Foucalut, 1979, p. 21) suggesting that author has been a main part of the literary criticism.

Even without considering this Foucaultian discourse, it is almost intuitive that talking about the author of the books can be an indicator of a higher level of discussions about a book. For example, if we are discussing _The Sound and the Fury,_ a review that mentions William Faulkner's stream-of-consciousness style, his recurring theme of the collapse of the South and the _Yoknapatawpha_ county will be much more sophisticated than a review that only talks about the book itself.

To test this relationship, I calculated the proportion of reviews that explicitly mentions the author's last name at least once for each book. Note that an author's last name, "Ng" showed matched for too many words that are not referencing the author (e.g. -ing), so a book was discarded from the analysis. Following is the boxplot showing the proportion of author-mentioning reviews for each book:

![Figure6]()

The above plot and t-test results show that reviews for high culture books mention the name of the authors significantly more (t=2.06, p=.04). This further corroborates that reviews of high culture books are showing the semiotic relationship discussed before.

## Topics: Is There a Topic That is Selectively Appearing on One Category?

Finally, I will use the topic modeling method Latent Dirichlet Allocation (LDA) to see what kind of topics are appearing in the reviews. More specifically, I will investigate if there is a topic that almost selectively appears in one of the categories, and if so, try to put a label on what that topic means. 

Although the coherence score analysis suggested that the highest coherence score will be reached if we use 21 topics in the LDA, I chose 5 topics to find because of the interpretability of the plots. The following plots shows the average proportion of each topic in each book:

![Figure7]()
![Figure8]()

The above functions show that topic # 2 (shown in green) takes up a lot of portions in the reviews for high culture books, but it almost completely disappears in reviews for popular culture books. On the contrary, topic # 4 (shown in orange) seems to be taking a large proportion in reviews for popular culture books, but it takes a quite smaller proportion for the high culture books. t-tests shows that this difference is significant (topic #2: t=15.31, p<.001, topic #4: t=-9.00, p<.001).

So what could these topics be? Let's layout the top 10 words that consist of each topic:

| **Topic #2** | Topic #4  |
| ------------ | --------- |
| novel        | book      |
| character    | read      |
| story        | like      |
| reader       | one       |
| one          | story     |
| book         | character |
| life         | get       |
| write        | really    |
| time         | reacher   |
| author       | time      |

We can see that there is some overlap between the words consisting of the topics, like 'character', 'story' and 'book'. These are elements of a novel - so we can guess that they are talking about some kind of literary elements of the books. However, topic #2 has the words 'novel' and 'author', which were some words we discussed that could signify a more educated discourse in literature. Based on this, I interpret that topic #2 is a topic about more advanced discussions about literature, while topic #4 is a relatively light discussion about the books. This result confirms the final semiotic relationship we will discuss - talking about more advanced literature topics is the Sign-Vehicle and the Object and Interpretant being the same with the last two we discussed.

# Black-box Models

## Using Random Forest to Classify the Reviews

A whole different approach to this analysis will be to look at the data as some kind of classification problem data set. In other words, we can see each review as features that we can use to predict the classification - high culture or popular culture. I trained a Random Forest model based on half of the reviews and evaluate performance on the other half. I broke down each review into tf-idf vector and feed half of it to the model in a supervised way. Then, I saw how well this model performs in the other half which the model did not see  (test set 1).

However, using tf-idf vectors in this model could be very prone to a generalizability problem. For example, the model could learn that if a review contains the word 'fitzgerald', it is a review that belongs to the group of reviews of high culture books. Therefore, to test if this model can generalize outside the set of books and authors in the data set, I collected reviews for 10 new books to test the model on (test set 2). For the high culture books, I collected books written by three recent Nobel laureates in literature who wrote in English (Doris Lessing, Alice Munro, Kazuo Ishiguro; but note that Alice Munro might not be classified as a novel writer) and two recent Booker Prize-winning books (_Girl, Woman, Other_ and _Milkman_ ). For the popular culture books, I collected top books that did not make it initially from the New York Times list that did not have overlapping authors (_Summer of '69, The Abbey, The Lost Girls of Paris, Lilac Girls, Takedown Twenty_ ).

The train set accuracy and AUC reached 1, showing perfect performance. The performance was exceptionally good for test set 1, which showed an accuracy of 0.92 and an AUC of 0.92. As expected, the performance was a little lower for test set 2, showing an accuracy of 0.77 and an AUC of 0.78.
Following plot shows the confusion matrix for each set:
![Figure9]()

We can see from the above result that the model does incredibly well in predicting the train set and test set consisting of the books that also appears in the train set, but it does quite worse when predicting a test set that consist of books and authors that the model has never seen before. This shows that the generalizability problem does exist in the black-box model. Perhaps ruling out all the author names and character names in the reviews when building the tf-idf vector could be a better way to make the model more generalizable (but this might reduce the performance in the test set 1).

However, even in the test set that consists of books and authors that the model has never seen before, it shows a pretty decent AUC of 0.78. Although it is difficult to interpret what is going on inside the model (unless we examine every single tree in the forest), this serves as another proof that there is a different pattern happening in the reviews of each category.

## Using RNN to generate reviews-like texts

Finally, using reviews for high culture books and reviews for popular culture books separately, I trained two separate Recurrent Neural Networks (RNN; Sutskever, Martens & Hinton, 2011). Following are some generated texts from the RNNs with `temperature=0.5`:

### Trained on Reviews of High Culture Books

1. this is one of those books that i was reading this book before i finished it and i guess the book is a fan of the plot and the main characters are the main characters who love it and then all the wa

2. a story of a man who has an action of contrary the book and the last time i was reading this book. the novel is a very sense of the middle of the book. i find it so that some of the characters are a

3. i hope the first time i was writing a book with a whole day of the 1930's years ago, but i was to say that it makes me want to see anything that is written with some of the most confusing character 

4. oh, victorian literature or maggie's nowca for the beast at the lighthouse. german, so it deserves wildo-were true readers of making, which later usually is happening including. what i get to say th

### Trained on Reviews on Popular Culture Books

1. i loved it. i just couldn't put it down. i was reading a lot of the second book so i felt like the author was trying to do the story that i could tell you this is a book that will be a strike and co

2. i have said the first third of the book itself is married to a concept of the complex conspiracy instead of with a murder in contrast. i think the ending was slow and i think they are the best way t

3. many others of this book is full of special police with cora and her family. maria and tell the story of the characters that are all so depressing and depressing. i will say that i didnt see my life

4. this book is a completely different thrillers. the book is familiar and a collection of characters who were still in the story. i will say that i could have been writing more of the villains of the 

The models do not quite seem to reach the human level of producing reviews. However, I do note that I only trained the RNNs through 10 epochs because of time constraints. It might perform better if I used more epochs.

# Conclusion

In this analysis, I inspected the people's digital cultural patterns towards books of high culture and books of popular culture. I first saw that replication of a broader legisign is happening on the website Goodreads since people are consistently replicating the act of writing book reviews on the website. However, there were differences between reviews about high culture books and popular culture books within the replication of legisigns. When reviewing the books from high culture, people repeatedly indexed that they have more educated backgrounds through various Sign-Vehicles. These Sign-Vehicles included using more complicated words and longer sentences (readability metrics analysis), more specific language ("book" versus "novel" analysis), talking more about the authors (author analysis), and talking about topics related to literary criticism (topic modeling analysis). All in all, these results seem to be in line with Bourdieu’s theory of taste.

In addition, I utilized some black-box models to see if they can classify and predict which category the review belongs to (random forest model) or if they can generate texts that are similar to the reviews (recurrent neural network model). Both models did not function as well as we want it to be, but the results did seem to suggest that if we could use more advanced techniques or feed-in better-processed data, they could do sufficiently well.

One question that was unanswered in this analysis was whether this difference is between-group differences or within-individual differences. In other words, I did not identify if the differences in reviews were caused simply by a different group of people reviewing different categories, or the differences were caused by people shifting their review style depending on what book they were reviewing. Bourdieu’s theory suggests that there will be different groups of people showing different behaviors, so it will be interesting to see if this can be empirically shown.

# References

_About Goodreads._ (n.d.). Goodreads. Retrieved February 10th, 2020

_About Modern Library._ (n.d.). Modern Library. Retrieved February 10th, 2020, from http://www.modernlibrary.com/about/

Bennett, T. (1980). Popular culture: a teaching object. _Screen Education, 34_ (18), 17-29.

Blewitt, J. (1993). Film, ideology and Bourdieu's critique of public taste. _The British Journal of Aesthetics, 33_ (4), 367-373.

Bloom, H. (2014). _The Western Canon: The Books and School of the Ages_. Houghton Mifflin Harcourt.

Bourdieu, P. (1984). _Distinction: A Social Critique of the Judgement of Taste_ (R. Nice, Trans.). Harvard University Press. (Original work published 1979)

Casement, W. (2002). Whither the great books?. _Academic Questions, 15_ (4), 36.

Coleman, M., & Liau, T. L. (1975). A computer readability formula designed for machine scoring. _Journal of Applied Psychology, 60_ (2), 283.

Du Gay, P., Hall, S., Janes, L., Madsen, A. K., Mackay, H., & Negus, K. (2013). _Doing cultural studies: The story of the Sony Walkman._ Sage.

Friedman, D. B., & Hoffman-Goetz, L. (2006). A systematic review of readability and comprehension instruments used for print and web-based cancer information. _Health Education & Behavior, 33_ (3), 352-373.

Foucault, M. (1979). Authorship: what is an author?. _Screen, 20_ (1), 13-34.

Gilkerson, J., Richards, J. A., Warren, S. F., Montgomery, J. K., Greenwood, C. R., Kimbrough Oller, D., ... & Paul, T. D. (2017). Mapping the early language environment using all-day recordings and automated analysis. _American Journal of Speech-Language Pathology, 26_ (2), 248-265.

Hart, B., & Risley, T. R. (1995). _Meaningful differences in the everyday experience of young American children._ Paul H Brookes Publishing.

Hoff, E. (2003). The specificity of environmental influence: Socioeconomic status affects early vocabulary development via maternal speech. _Child development, 74_ (5), 1368-1378.

Levine, L. W. (1990). _Highbrow/lowbrow: The Emergence of Cultural Hierarchy in America_ (Vol. 3). Harvard University Press.

Milton, J., & Treffers-Daller, J. (2013). _Vocabulary size revisited: the link between vocabulary size and academic achievement. Applied Linguistics Review, 4_ (1), 151-172.

Murphy, M. J. (2015, July 16). The Best-Seller List 55 Years Ago. _The New York Times._ https://www.nytimes.com/2015/07/17/books/the-best-seller-list-55-years-ago.html

Nehamas, A. (1986). What an author is. _The Journal of Philosophy, 83_ (11), 685-691.

Rahwan, I., Cebrian, M., Obradovich, N., Bongard, J., Bonnefon, J. F., Breazeal, C., ... & Jennings, N. R. (2019). Machine behaviour. _Nature, 568_ (7753), 477-486.

Rowe, M. L. (2008). Child-directed speech: Relation to socioeconomic status, knowledge of child development and child vocabulary skill. _Journal of child language, 35_ (1), 185-205.

Storey, J. (2009). _Cultural theory and popular culture: An introduction_ (5th ed.). Pearson Longman.

Sutskever, I., Martens, J., & Hinton, G. E. (2011). Generating text with recurrent neural networks. In _Proceedings of the 28th international conference on machine learning (ICML-11)_ (pp. 1017-1024).


